# Decode Latency æµ‹é‡æ–¹æ³•æ”¹è¿›æ–¹æ¡ˆ

> **æ–‡æ¡£ç›®çš„**: æ·±åº¦åˆ†ædecode latencyæµ‹é‡æ–¹æ³•ï¼Œå®ç°æœ€ä½³æµ‹é‡æ–¹æ¡ˆ  
> **æœ€åæ›´æ–°**: 2026-01-08

## ğŸ“‹ ç›®å½•

1. [å½“å‰å®ç°åˆ†æ](#å½“å‰å®ç°åˆ†æ)
2. [é—®é¢˜åˆ†æ](#é—®é¢˜åˆ†æ)
3. [æœ€ä½³æ–¹æ¡ˆ](#æœ€ä½³æ–¹æ¡ˆ)
4. [å®æ–½è®¡åˆ’](#å®æ–½è®¡åˆ’)

---

## ğŸ” å½“å‰å®ç°åˆ†æ

### ç°çŠ¶

**ä¸¤ç§å®ç°æ–¹å¼å¹¶å­˜**:

1. **`base_experiment.py`**: ä½¿ç”¨`_measure_with_hooks`æ–¹æ³•ï¼Œ**ç›´æ¥æµ‹é‡**decodeæ—¶é—´
   - âœ… åœ¨`tracked_forward`ä¸­è®°å½•ç¬¬ä¸€ä¸ªdecode stepçš„å¼€å§‹æ—¶é—´
   - âœ… åœ¨`generate()`å®Œæˆåè®°å½•decodeç»“æŸæ—¶é—´
   - âœ… ç›´æ¥è®¡ç®—ï¼š`T_LLM_decode = decode_end_time - decode_start_time`
   - âœ… ç„¶åè®¡ç®—ï¼š`T_decode_per_token = T_LLM_decode / output_tokens`

2. **`motivate/base_experiment.py`å’Œ`exp6_accuracy.py`**: ä½¿ç”¨**å‡æ³•æ–¹æ³•**
   - âŒ `T_LLM_decode = max(0.0, T_total - T_vision_total - T_LLM_prefill)`
   - âŒ å¯¼è‡´æµ‹é‡è¯¯å·®ç´¯ç§¯
   - âŒ 22.85%çš„æ ·æœ¬å‡ºç°`T_LLM_decode = 0.0`ä½†`output_tokens > 0`

### ä»£ç ä½ç½®

**ç›´æ¥æµ‹é‡æ–¹æ³•** (`experiments/base_experiment.py:572-640`):
```python
# åœ¨tracked_forwardä¸­è®°å½•decodeå¼€å§‹æ—¶é—´
if not is_prefill and decode_start_time is None:
    if self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)
    decode_start_time = time.perf_counter()

# åœ¨generate()å®Œæˆåè®°å½•decodeç»“æŸæ—¶é—´
if decode_start_time is not None:
    if self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)
    decode_end_time = time.perf_counter()
    decode_times.append((decode_end_time - decode_start_time) * 1000)
```

**å‡æ³•æ–¹æ³•** (`experiments/motivate/base_experiment.py:538`):
```python
results["T_LLM_decode"] = max(0.0, results["T_total"] - results.get("T_vision_total", 0.0) - results.get("T_LLM_prefill", 0.0))
```

---

## âš ï¸ é—®é¢˜åˆ†æ

### ä¸ºä»€ä¹ˆå‡æ³•æ–¹æ³•æœ‰é—®é¢˜ï¼Ÿ

1. **æµ‹é‡ç¯å¢ƒä¸ä¸€è‡´**:
   - Visionå’ŒPrefillåˆ†åˆ«æµ‹é‡æ—¶ï¼ŒGPUå¯èƒ½æœ‰ç¼“å­˜ï¼Œæµ‹é‡è¾ƒå¿«
   - `T_total`æµ‹é‡å‰è°ƒç”¨`empty_cache()`ï¼Œç¼“å­˜è¢«æ¸…ç©ºï¼Œæµ‹é‡è¾ƒæ…¢
   - å¯¼è‡´`T_vision_total + T_LLM_prefill > T_total`ï¼Œ`T_LLM_decode`ä¸ºè´Ÿæ•°

2. **è¯¯å·®ç´¯ç§¯**:
   - æ¯ä¸ªç»„ä»¶çš„æµ‹é‡è¯¯å·®ä¼šç´¯ç§¯
   - Visionè¢«è®¡ç®—äº†3æ¬¡ï¼ˆåˆ†åˆ«æµ‹é‡visionã€prefillã€totalï¼‰ï¼Œæ¯æ¬¡æ—¶é—´ä¸åŒ
   - å¯¹äºçŸ­è¾“å‡ºï¼Œè¯¯å·®å æ¯”æ›´å¤§

3. **æ•°æ®è´¨é‡é—®é¢˜**:
   - 22.85%çš„æ ·æœ¬`T_LLM_decode = 0.0`ä½†`output_tokens > 0`
   - å¯¼è‡´`T_decode_per_token = 0.0`ï¼Œæ•°æ®ä¸å¯ç”¨

### ä¸ºä»€ä¹ˆç›´æ¥æµ‹é‡æ–¹æ³•æ›´å¥½ï¼Ÿ

1. **å‡†ç¡®æ€§**:
   - ç›´æ¥æµ‹é‡decodeé˜¶æ®µçš„æ€»æ—¶é—´ï¼Œä¸ä¾èµ–å…¶ä»–ç»„ä»¶çš„æµ‹é‡
   - é¿å…äº†è¯¯å·®ç´¯ç§¯
   - æµ‹é‡ç¯å¢ƒä¸€è‡´ï¼ˆåœ¨åŒä¸€ä¸ª`generate()`è°ƒç”¨ä¸­ï¼‰

2. **ç®€å•æ€§**:
   - åªéœ€è¦è®°å½•ä¸¤ä¸ªæ—¶é—´ç‚¹ï¼šdecodeå¼€å§‹å’Œç»“æŸ
   - ç„¶åé™¤ä»¥`output_tokens`å¾—åˆ°per-token latency
   - é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤

3. **å¯é æ€§**:
   - ä¸ä¾èµ–å‡æ³•è®¡ç®—ï¼Œä¸ä¼šå‡ºç°è´Ÿæ•°
   - å³ä½¿æµ‹é‡æœ‰è¯¯å·®ï¼Œä¹Ÿæ˜¯ç›´æ¥è¯¯å·®ï¼Œä¸ä¼šæ”¾å¤§

---

## âœ… æœ€ä½³æ–¹æ¡ˆ

### æ–¹æ¡ˆï¼šç»Ÿä¸€ä½¿ç”¨ç›´æ¥æµ‹é‡æ–¹æ³•

**æ ¸å¿ƒæ€æƒ³**:
1. **ç›´æ¥æµ‹é‡decodeæ€»æ—¶é—´**: åœ¨`generate()`å†…éƒ¨ï¼Œè®°å½•ç¬¬ä¸€ä¸ªdecode stepçš„å¼€å§‹æ—¶é—´å’Œæœ€åä¸€ä¸ªdecode stepçš„ç»“æŸæ—¶é—´
2. **è®¡ç®—per-token latency**: `T_decode_per_token = T_LLM_decode / output_tokens`

**å®ç°è¦ç‚¹**:

1. **åœ¨`tracked_forward`ä¸­è®°å½•decodeå¼€å§‹æ—¶é—´**:
   ```python
   if not is_prefill and decode_start_time is None:
       # ç¬¬ä¸€ä¸ªdecode step
       if self.device.type == 'cuda':
           torch.cuda.synchronize(self.device)
       decode_start_time = time.perf_counter()
   ```

2. **åœ¨`generate()`å®Œæˆåè®°å½•decodeç»“æŸæ—¶é—´**:
   ```python
   if decode_start_time is not None:
       if self.device.type == 'cuda':
           torch.cuda.synchronize(self.device)
       decode_end_time = time.perf_counter()
       T_LLM_decode = (decode_end_time - decode_start_time) * 1000
   ```

3. **è®¡ç®—per-token latency**:
   ```python
   output_tokens = output.shape[1] - input_ids.shape[1]
   T_decode_per_token = T_LLM_decode / output_tokens if output_tokens > 0 else 0.0
   ```

### ä¼˜åŠ¿

1. **å‡†ç¡®æ€§**: ç›´æ¥æµ‹é‡ï¼Œæ— è¯¯å·®ç´¯ç§¯
2. **ç®€å•æ€§**: é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºå®ç°
3. **å¯é æ€§**: ä¸ä¼šå‡ºç°è´Ÿæ•°æˆ–0å€¼ï¼ˆé™¤éçœŸçš„æ²¡æœ‰decodeï¼‰
4. **ä¸€è‡´æ€§**: æ‰€æœ‰å®éªŒä½¿ç”¨ç›¸åŒçš„æµ‹é‡æ–¹æ³•

---

## ğŸš€ å®æ–½è®¡åˆ’

### æ­¥éª¤1: æ›´æ–°`motivate/base_experiment.py`

**å½“å‰**: ä½¿ç”¨å‡æ³•æ–¹æ³•
**ç›®æ ‡**: ä½¿ç”¨`_measure_with_hooks`æ–¹æ³•ï¼ˆç»§æ‰¿è‡ª`base_experiment.py`ï¼‰

**æ£€æŸ¥**: `motivate/base_experiment.py`æ˜¯å¦ç»§æ‰¿è‡ª`base_experiment.py`ï¼Ÿ

### æ­¥éª¤2: æ›´æ–°`exp6_accuracy.py`

**å½“å‰**: ä½¿ç”¨å‡æ³•æ–¹æ³•
**ç›®æ ‡**: ä½¿ç”¨ç›´æ¥æµ‹é‡æ–¹æ³•ï¼ˆç±»ä¼¼`_measure_with_hooks`ï¼‰

**å®ç°**: åœ¨`exp6_accuracy.py`ä¸­å®ç°ç±»ä¼¼çš„hookæœºåˆ¶ï¼Œç›´æ¥æµ‹é‡decodeæ—¶é—´

### æ­¥éª¤3: éªŒè¯

1. è¿è¡Œå®éªŒï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰`T_LLM_decode = 0.0`ä½†`output_tokens > 0`çš„æƒ…å†µ
2. æ¯”è¾ƒæ–°æ—§æ–¹æ³•çš„æµ‹é‡ç»“æœ
3. ç¡®è®¤per-token latencyçš„åˆ†å¸ƒæ˜¯å¦æ›´åˆç†

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ”¹è¿›å‰ï¼ˆå‡æ³•æ–¹æ³•ï¼‰

- âŒ 22.85%çš„æ ·æœ¬`T_LLM_decode = 0.0`
- âŒ Decode per-token latencyä¸output_tokensæœ‰0.70çš„ç›¸å…³æ€§ï¼ˆæµ‹é‡è¯¯å·®å¯¼è‡´ï¼‰
- âŒ çŸ­è¾“å‡ºçš„æµ‹é‡è¯¯å·®æ›´å¤§ï¼ˆ6.09% vs 2.00%ï¼‰

### æ”¹è¿›åï¼ˆç›´æ¥æµ‹é‡æ–¹æ³•ï¼‰

- âœ… æ‰€æœ‰æœ‰decodeçš„æ ·æœ¬éƒ½æœ‰æœ‰æ•ˆçš„`T_LLM_decode`
- âœ… Decode per-token latencyåº”è¯¥åªä¸é…ç½®ç›¸å…³ï¼Œä¸output_tokensæ— å…³
- âœ… æµ‹é‡è¯¯å·®æ›´å°ï¼Œæ›´ç¨³å®š

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **[key_insights_latency_measurement.md](key_insights_latency_measurement.md)**: æµ‹é‡å…³é”®æ´å¯Ÿ
- **[latency_measurement_refactoring.md](latency_measurement_refactoring.md)**: æµ‹é‡é‡æ„æ–¹æ¡ˆ
- **[decode_measurement_strategy.md](decode_measurement_strategy.md)**: Decodeæµ‹é‡ç­–ç•¥

---

**æœ€åæ›´æ–°**: 2026-01-08  
**ç»´æŠ¤è€…**: Analysis Team



