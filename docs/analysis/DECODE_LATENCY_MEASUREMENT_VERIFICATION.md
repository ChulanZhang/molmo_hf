# Decode Latency æµ‹é‡æ–¹æ³•éªŒè¯

> **æ–‡æ¡£ç›®çš„**: éªŒè¯å½“å‰decode latencyæµ‹é‡æ–¹æ³•çš„æ­£ç¡®æ€§  
> **æœ€åæ›´æ–°**: 2026-01-08

## âœ… å½“å‰å®ç°éªŒè¯

### `acc_lat_profiling.py`ä½¿ç”¨çš„æµ‹é‡æ–¹æ³•

`acc_lat_profiling.py`è°ƒç”¨`self.measure_inference_latency()`ï¼Œè¯¥æ–¹æ³•æ¥è‡ª`BaseExperiment`ï¼ˆ`base_experiment.py`ï¼‰ï¼Œä½¿ç”¨`_measure_with_hooks`æ–¹æ³•**ç›´æ¥æµ‹é‡decodeæ—¶é—´**ã€‚

### å®ç°ç»†èŠ‚ï¼ˆ`base_experiment.py:572-661`ï¼‰

**1. è®°å½•decodeå¼€å§‹æ—¶é—´**ï¼ˆç¬¬ä¸€ä¸ªdecode stepï¼‰:
```python
def tracked_forward(*args, **kwargs):
    nonlocal forward_count, vision_start_time, decode_start_time
    is_prefill = forward_count == 0
    
    # Record decode start time (only on first decode step)
    if not is_prefill and decode_start_time is None:
        if self.device.type == 'cuda':
            torch.cuda.synchronize(self.device)
        decode_start_time = time.perf_counter()
    
    # Call original forward
    output = original_forward(*args, **kwargs)
    
    # Increment forward count after prefill
    if is_prefill:
        forward_count += 1
    
    return output
```

**2. è®°å½•decodeç»“æŸæ—¶é—´**ï¼ˆæ‰€æœ‰decode stepså®Œæˆåï¼‰:
```python
# After model.generate() completes
if decode_start_time is not None:
    if self.device.type == 'cuda':
        torch.cuda.synchronize(self.device)
    decode_end_time = time.perf_counter()
    decode_times.append((decode_end_time - decode_start_time) * 1000)
```

**3. è®¡ç®—å¹³å‡å€¼**:
```python
if decode_times:
    results["T_LLM_decode"] = np.mean(decode_times)
```

**4. è®¡ç®—per-token latency**ï¼ˆåœ¨`acc_lat_profiling.py:1240`ï¼‰:
```python
"T_decode_per_token": latency_results.get("T_LLM_decode", 0.0) / max(num_output_tokens, 1)
```

---

## âœ… ç»“è®º

**å½“å‰å®ç°å·²ç»æ˜¯æ­£ç¡®çš„ï¼**

1. **ç›´æ¥æµ‹é‡**: ä¸æ˜¯å‡æ³•ï¼Œè€Œæ˜¯ç›´æ¥æµ‹é‡decodeé˜¶æ®µçš„æ€»æ—¶é—´
2. **å‡†ç¡®è®¡ç®—**: `T_LLM_decode = decode_end_time - decode_start_time`
3. **Per-tokenè®¡ç®—**: `T_decode_per_token = T_LLM_decode / output_tokens`

è¿™æ­£æ˜¯ç”¨æˆ·å»ºè®®çš„æ–¹æ³•ï¼š**ç›´æ¥ç»Ÿè®¡total decode latencyï¼Œç„¶åé™¤ä»¥output tokenæ•°**ã€‚

---

## ğŸ“Š ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•æ›´å¥½ï¼Ÿ

### ä¼˜åŠ¿

1. **å‡†ç¡®æ€§**: ç›´æ¥æµ‹é‡ï¼Œæ— è¯¯å·®ç´¯ç§¯
2. **ç®€å•æ€§**: é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£
3. **å¯é æ€§**: ä¸ä¼šå‡ºç°è´Ÿæ•°æˆ–0å€¼ï¼ˆé™¤éçœŸçš„æ²¡æœ‰decodeï¼‰
4. **ä¸€è‡´æ€§**: æ‰€æœ‰å®éªŒä½¿ç”¨ç›¸åŒçš„æµ‹é‡æ–¹æ³•

### ä¸å‡æ³•æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å‡†ç¡®æ€§ | è¯¯å·®ç´¯ç§¯ | å¯é æ€§ |
|------|--------|---------|--------|
| **ç›´æ¥æµ‹é‡**ï¼ˆå½“å‰ï¼‰ | âœ… é«˜ | âœ… æ—  | âœ… é«˜ |
| å‡æ³•æ–¹æ³• | âŒ ä½ | âŒ æœ‰ | âŒ ä½ |

---

## ğŸ” éªŒè¯æ£€æŸ¥

### æ£€æŸ¥ç‚¹1: æ˜¯å¦ä½¿ç”¨ç›´æ¥æµ‹é‡

âœ… **å·²ç¡®è®¤**: `base_experiment.py`ä½¿ç”¨`_measure_with_hooks`æ–¹æ³•ï¼Œç›´æ¥æµ‹é‡decodeæ—¶é—´

### æ£€æŸ¥ç‚¹2: æ˜¯å¦ä½¿ç”¨å‡æ³•

âœ… **å·²ç¡®è®¤**: æœªå‘ç°å‡æ³•è®¡ç®—æ–¹æ³•ï¼ˆ`T_LLM_decode = T_total - T_vision - T_prefill`ï¼‰

### æ£€æŸ¥ç‚¹3: Per-tokenè®¡ç®—

âœ… **å·²ç¡®è®¤**: `acc_lat_profiling.py:1240`æ­£ç¡®è®¡ç®—ï¼š`T_decode_per_token = T_LLM_decode / output_tokens`

---

## ğŸ“ æ€»ç»“

**å½“å‰å®ç°å·²ç»å®Œå…¨ç¬¦åˆæœ€ä½³å®è·µ**ï¼š

1. âœ… ç›´æ¥æµ‹é‡decodeæ€»æ—¶é—´ï¼ˆä¸æ˜¯å‡æ³•ï¼‰
2. âœ… é™¤ä»¥output_tokenså¾—åˆ°per-token latency
3. âœ… æ— è¯¯å·®ç´¯ç§¯
4. âœ… é€»è¾‘æ¸…æ™°

**æ— éœ€ä¿®æ”¹**ï¼Œå½“å‰å®ç°å·²ç»æ˜¯æ­£ç¡®ä¸”æœ€ä¼˜çš„ã€‚

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **[DECODE_LATENCY_MEASUREMENT_IMPROVEMENT.md](DECODE_LATENCY_MEASUREMENT_IMPROVEMENT.md)**: æ”¹è¿›æ–¹æ¡ˆæ–‡æ¡£ï¼ˆé’ˆå¯¹å…¶ä»–ä½¿ç”¨å‡æ³•æ–¹æ³•çš„å®éªŒï¼‰
- **[key_insights_latency_measurement.md](key_insights_latency_measurement.md)**: æµ‹é‡å…³é”®æ´å¯Ÿ

---

**æœ€åæ›´æ–°**: 2026-01-08  
**ç»´æŠ¤è€…**: Analysis Team


