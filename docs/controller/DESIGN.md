# Controllerè®¾è®¡æ–‡æ¡£ï¼ˆç»Ÿä¸€ç‰ˆï¼‰

> **æ–‡æ¡£çŠ¶æ€**: æœ¬æ–‡æ¡£æ•´åˆäº†æ‰€æœ‰controllerè®¾è®¡æ–‡æ¡£ï¼ŒåŸºäºç°æœ‰ä»£ç å®ç°å’ŒSIGMETRICSæ ‡å‡†ã€‚
> **æœ€åæ›´æ–°**: 2026-01-01
> **ç‰ˆæœ¬**: 2.0

## ç›®å½•

1. [è®¾è®¡æ¦‚è¿°](#1-è®¾è®¡æ¦‚è¿°)
2. [ç³»ç»Ÿçº¦æŸä¸æ‰§è¡Œæµç¨‹](#2-ç³»ç»Ÿçº¦æŸä¸æ‰§è¡Œæµç¨‹)
3. [æ¶æ„è®¾è®¡](#3-æ¶æ„è®¾è®¡)
4. [Knobè®¾è®¡ç»†èŠ‚](#4-knobè®¾è®¡ç»†èŠ‚)
5. [è¾“å…¥ç‰¹å¾è®¾è®¡](#5-è¾“å…¥ç‰¹å¾è®¾è®¡)
6. [è®­ç»ƒæ–¹æ³•](#6-è®­ç»ƒæ–¹æ³•)
7. [Overheadåˆ†æ](#7-overheadåˆ†æ)
8. [å®ç°ç»†èŠ‚](#8-å®ç°ç»†èŠ‚)
9. [æ€§èƒ½æŒ‡æ ‡](#9-æ€§èƒ½æŒ‡æ ‡)
10. [å…³é”®è®¾è®¡å†³ç­–](#10-å…³é”®è®¾è®¡å†³ç­–)

---

## 1. è®¾è®¡æ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡

**SIGMETRICSæ ‡å‡†**ï¼š
1. **ä½Overhead**: Controllerå¼€é”€ <0.1% of total inference
2. **é«˜æ•ˆæ€§**: å†³ç­–æ—¶é—´ <0.2ms
3. **æœ‰æ•ˆæ€§**: æ˜¾è‘—æå‡accuracy-latency trade-off
4. **ç®€æ´æ€§**: è®¾è®¡ç®€å•ï¼Œæ˜“äºéƒ¨ç½²
5. **å¯æ‰©å±•æ€§**: é€‚ç”¨äºä¸åŒç¡¬ä»¶å’Œæ¨¡å‹è§„æ¨¡

### 1.2 æ ¸å¿ƒè®¾è®¡ç†å¿µ

**ä¸¤é˜¶æ®µé¢„æµ‹æ¶æ„**ï¼š
- **Stage 1**: åœ¨vision encoderä¹‹å‰é¢„æµ‹Knob1ï¼ˆvision tokens tierï¼‰
- **Stage 2**: åœ¨vision encoderä¹‹åé¢„æµ‹Knob2 & Knob3ï¼ˆMoE top-Kå’Œtransformer blocksï¼‰

**å…³é”®åŸåˆ™**ï¼š
- æœ€å°åŒ–controllerå¼€é”€
- ç¬¦åˆç³»ç»Ÿæ‰§è¡Œæµç¨‹çº¦æŸ
- ä½¿ç”¨importance-based pruningç®€åŒ–Knob3

### 1.3 ä¸‰ä¸ªKnobçš„æœ€ç»ˆè®¾è®¡

| Knob | æ§åˆ¶å†…å®¹ | å†³ç­–æ—¶æœº | å®ç°æ–¹å¼ | è¾“å‡ºç©ºé—´ |
|------|---------|---------|---------|---------|
| **Knob1** | Vision tokens tier (low/medium/high) | Before vision encoder | Stage 1 predictor | 3 choices |
| **Knob2** | MoE top-K (4/6/8/10/12) | After vision encoder | Stage 2 predictor | 5 choices |
| **Knob3** | Transformer blocks count (8/10/12/14/16) | After vision encoder | **Importance-based pruning** (top-N) | 5 choices |

**å…³é”®æ”¹è¿›**ï¼š
- Knob3ä»maské¢„æµ‹ï¼ˆ2^16ï¼‰ç®€åŒ–ä¸ºnum_blocksé¢„æµ‹ï¼ˆ5ï¼‰ï¼ŒåŸºäºé¢„è®¡ç®—çš„importance score
- Block selectionæ˜¯ç¡®å®šæ€§çš„ï¼Œä¸ä¾èµ–è¾“å…¥

---

## 2. ç³»ç»Ÿçº¦æŸä¸æ‰§è¡Œæµç¨‹

### 2.1 ç³»ç»Ÿçº¦æŸ

**VLMæ¶æ„**: `Vision Encoder â†’ Projector â†’ LLM`

**å…³é”®çº¦æŸ**ï¼š
1. **Knob1**: å¿…é¡»åœ¨vision encoderä¹‹å‰å†³å®š
   - Cropæ•°é‡å†³å®šå›¾åƒå¤„ç†æ–¹å¼ï¼ˆtiling, resizeï¼‰
   - ä¸€æ—¦è¿›å…¥vision encoderï¼Œcropæ•°é‡å°±å›ºå®šäº†
2. **Knob2 & Knob3**: å¿…é¡»åœ¨LLMå‰å‡ å±‚æˆ–ä¹‹å‰å†³å®š
   - Top-Kå’Œblockså½±å“åç»­è®¡ç®—
   - é¿å…é‡å¤è®¡ç®—

### 2.2 æ‰§è¡Œæµç¨‹

```
1. Input: Image + Prompt + Latency Budget
    â†“
2. Stage 1: Predict Knob1
   - Extract: Language Feature (from prompt)
   - Extract: Budget Feature
   - Predict: Vision Tokens Tier (low/medium/high)
   - Overhead: ~0.01-0.1ms
    â†“
3. Image Preprocessing (based on Knob1)
   - Determine crop count from tier
   - Apply tiling and resize
    â†“
4. Vision Encoding
   - Vision Encoder: Process crops
   - Projector: Map to LLM space
   - Extract: Vision Feature (pooled) for Stage 2
    â†“
5. Stage 2: Predict Knob2 & Knob3
   - Extract: Vision Feature (from encoder+projector, pooled)
   - Extract: Language Feature (from prompt)
   - Extract: Budget Feature
   - Predict: MoE Top-K + Transformer Blocks
   - Overhead: ~0.1ms
    â†“
6. Apply Knobs to LLM
   - Set top_k for MoE layers (zero overhead, attribute modification)
   - Select blocks by importance (deterministic, O(n log n))
   - Apply block mask
    â†“
7. LLM Forward (with adaptive knobs)
   - Generate output
```

### 2.3 è®­ç»ƒæ—¶æµç¨‹ï¼ˆRLæ–¹æ³•ï¼‰

```
1. Controller predicts knob configuration
   - Stage 1: Predict Knob1 (tier)
   - Process images with Knob1
   - Vision encoding
   - Stage 2: Predict Knob2 & Knob3
    â†“
2. Estimate latency (using Latency Estimator)
   - Input: knob configuration + token counts
   - Output: Estimated latency (enables large batch size)
    â†“
3. Execute model (real execution)
   - Use predicted knobs
   - Get accuracy (can use large batch size)
    â†“
4. Compute reward
   - accuracy + latency constraints
    â†“
5. Update controller (GRPO)
```

---

## 3. æ¶æ„è®¾è®¡

### 3.1 ä¸¤é˜¶æ®µæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Two-Stage Controller Architecture              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Stage 1: Knob1 Prediction (BEFORE Vision Encoder)        â”‚
â”‚  â”œâ”€ Input: Language Feature + Budget Feature               â”‚
â”‚  â”œâ”€ Network: Lightweight MLP                               â”‚
â”‚  â””â”€ Output: Vision Tokens Tier (low/medium/high)           â”‚
â”‚                                                              â”‚
â”‚  â†“ Image Preprocessing (based on Knob1)                   â”‚
â”‚  â†“ Vision Encoder + Projector                              â”‚
â”‚                                                              â”‚
â”‚  Stage 2: Knob2 & Knob3 Prediction (AFTER Projector)       â”‚
â”‚  â”œâ”€ Input: Vision Feature (encoder+projectorå) +            â”‚
â”‚  â”‚        Language Feature + Budget Feature                â”‚
â”‚  â”œâ”€ Network: Lightweight MLP (recommended)                â”‚
â”‚  â””â”€ Output: MoE Top-K + Transformer Blocks                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Stage 1: Knob1 Predictor

**è®¾è®¡é€‰é¡¹ï¼ˆä¸‰ä¸ªé€‰æ‹©ï¼Œå¤æ‚åº¦é€’å¢ï¼‰**ï¼š

#### é€‰é¡¹A: Budget-Onlyï¼ˆæœ€å°Overheadï¼Œæ¨èç”¨äºSIGMETRICSï¼‰
- **Input**: Latency Budget only
- **Network**: Tiny MLP (~10K params)
- **Overhead**: ~0.01ms
- **ä¼˜ç‚¹**: æœ€å°overheadï¼Œç®€å•ç›´æ¥
- **ç¼ºç‚¹**: æ— æ³•åˆ©ç”¨promptä¿¡æ¯
- **çŠ¶æ€**: âœ… ä¼˜å…ˆå®ç°

#### é€‰é¡¹B: Budget + Languageï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰
- **Input**: Language Feature + Budget Feature
- **Network**: Lightweight MLP (~50K params) æˆ– Semantic Router
- **Overhead**: ~0.1ms
- **ä¼˜ç‚¹**: å¯ä»¥åˆ©ç”¨promptä¿¡æ¯ï¼Œæ›´æ™ºèƒ½çš„å†³ç­–
- **ç¼ºç‚¹**: ç¨é«˜çš„overhead
- **çŠ¶æ€**: ğŸ”„ è°ƒç ”Semantic Routeré›†æˆ
- **è°ƒç ”æ–¹å‘**: å‚è€ƒ [Semantic Router](https://github.com/aurelio-labs/semantic-router) è¿›è¡Œå¿«é€Ÿè¯­ä¹‰è·¯ç”±å†³ç­–

#### é€‰é¡¹C: Budget + Language + Visionï¼ˆæœ€é«˜å¤æ‚åº¦ï¼‰
- **Input**: Vision Feature (global crop) + Language Feature + Budget Feature
- **Network**: MLPæˆ–Transformer
- **Overhead**: ~30-50msï¼ˆéœ€è¦é¢å¤–è¿è¡Œvision encoderï¼‰
- **ä¼˜ç‚¹**: æœ€å‡†ç¡®ï¼Œå¯ä»¥åˆ©ç”¨å›¾åƒå¤æ‚åº¦ä¿¡æ¯
- **ç¼ºç‚¹**: éœ€è¦å¤šè¿‡ä¸€évision encoderï¼Œoverheadå¤§
- **çŠ¶æ€**: âš ï¸ éœ€è¦ä¼˜åŒ–å»ºè®®
- **ä¼˜åŒ–å»ºè®®**: 
  - è€ƒè™‘ä½¿ç”¨è½»é‡çº§vision encoderï¼ˆå¦‚MobileViTï¼‰
  - ç¼“å­˜global cropçš„vision feature
  - ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒå°æ¨¡å‹

**å½“å‰ä»£ç å®ç°**: é€‰é¡¹Bï¼ˆ`two_stage_controller.py`ï¼‰

**å®æ–½è®¡åˆ’**: ä»é€‰é¡¹Aå¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦ã€‚

### 3.3 Stage 2: Knob2 & Knob3 Predictor

**æ¶æ„**: ç‹¬ç«‹è½»é‡çº§MLPï¼ˆæ¨èï¼‰

```python
Vision Feature (B, d_model) + Language Feature (B, d_model) + Budget Feature (B, hidden_dim)
    â†“
Projections â†’ (B, hidden_dim) each
    â†“
Fusion (concat + MLP) â†’ (B, hidden_dim)
    â†“
Two Heads â†’ (B, 5) each [top_k: 4,6,8,10,12] [blocks: 8,10,12,14,16]
```

**å‚æ•°é‡**: ~50K-200K parametersï¼ˆå–å†³äºhidden_dimï¼‰

**å¤‡é€‰æ–¹æ¡ˆ**: ä½¿ç”¨LLMå‰å‡ å±‚ï¼ˆä¸æ¨èï¼Œoverheadå¤§ï¼‰

---

## 4. Knobè®¾è®¡ç»†èŠ‚

### 4.1 Knob1: Vision Tokens Tier

**æ§åˆ¶å†…å®¹**: Vision tokensçš„æ•°é‡ï¼ˆé€šè¿‡tieræ§åˆ¶cropæ•°é‡ï¼‰

**å†³ç­–æ—¶æœº**: å¿…é¡»åœ¨vision encoderä¹‹å‰

**å®ç°æ–¹å¼**: Stage 1 predictor

**è¾“å‡º**: 3ä¸ªé€‰æ‹©ï¼ˆlow, medium, highï¼‰

**è¯¦ç»†è®¾è®¡**: å‚è§`knob1_predictor_variants.md`

### 4.2 Knob2: MoE Top-K

**æ§åˆ¶å†…å®¹**: MoEå±‚çš„expertæ•°é‡

**å†³ç­–æ—¶æœº**: åœ¨vision encoderä¹‹åï¼ŒLLMä¹‹å‰

**å®ç°æ–¹å¼**: Stage 2 predictor

**è¾“å‡º**: 5ä¸ªé€‰æ‹©ï¼ˆ4, 6, 8, 10, 12ï¼‰

**åº”ç”¨æ–¹å¼**: ç›´æ¥ä¿®æ”¹`block.mlp.top_k`å±æ€§ï¼ˆé›¶overheadï¼‰

### 4.3 Knob3: Transformer Blocks

**æ§åˆ¶å†…å®¹**: æ¿€æ´»çš„transformer blockæ•°é‡

**å†³ç­–æ—¶æœº**: åœ¨vision encoderä¹‹åï¼ŒLLMä¹‹å‰ï¼ˆä¸¤é˜¶æ®µï¼‰æˆ–ä¸Knob1åŒæ—¶ï¼ˆä¸€é˜¶æ®µï¼‰

**å®ç°æ–¹å¼**: Importance-based pruning

**è¾“å‡º**: 5ä¸ªé€‰æ‹©ï¼ˆ8, 10, 12, 14, 16 blocksï¼‰

**Importance Scoreç†è§£**:
- **Data-Agnostic**: Importance scoreä¸æ•°æ®æ¥æºæ— å…³ï¼ˆcoco vqaå’Œtext vqaçš„importance scoreæ¥è¿‘ï¼‰
- **Task-Dependent**: Importance scoreä¸ä»»åŠ¡ç±»å‹ç›¸å…³ï¼ˆscience-qaä¸VQAä»»åŠ¡çš„importance scoreå·®è·è¾ƒå¤§ï¼‰
- **åº”ç”¨ç­–ç•¥**: 
  - å¯¹äºç›¸åŒä»»åŠ¡ç±»å‹ï¼Œå¯ä»¥ä½¿ç”¨ç»Ÿä¸€çš„importance score
  - å¯¹äºä¸åŒä»»åŠ¡ç±»å‹ï¼Œå¯èƒ½éœ€è¦ä»»åŠ¡ç‰¹å®šçš„importance scoreæˆ–åŠ¨æ€é€‰æ‹©

**å…³é”®è®¾è®¡**: 
- Controlleråªé¢„æµ‹num_blocksï¼ˆ5ä¸ªé€‰æ‹©ï¼‰
- Block selectionåŸºäºé¢„è®¡ç®—çš„importance scoreï¼ˆç¡®å®šæ€§ï¼‰
- ä¸éœ€è¦å­¦ä¹ maskï¼ˆ2^16ç§å¯èƒ½ï¼‰
- å¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„importance score

**å®ç°**:
```python
# Controlleré¢„æµ‹num_blocks
num_blocks = controller.predict_knob3(...)  # 8, 10, 12, 14, or 16

# æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©importance scoreï¼ˆå¯é€‰ï¼‰
task_type = infer_task_type(language_prompt)  # VQA, ScienceQA, etc.
importance_scores = get_importance_scores(task_type)

# åŸºäºimportance scoreé€‰æ‹©blocksï¼ˆç¡®å®šæ€§ï¼‰
selected_blocks = select_top_k_by_importance(importance_scores, num_blocks)

# åº”ç”¨mask
apply_block_mask(model, selected_blocks)
```

**ä¼˜åŠ¿**:
- è¾“å‡ºç©ºé—´ï¼š2^16 â†’ 5ï¼ˆå¤§å¹…ç®€åŒ–ï¼‰
- è®­ç»ƒç®€å•ï¼šåªéœ€è¦å­¦ä¹ 5ä¸ªé€‰æ‹©
- ç¨³å®šå¯é ï¼šåŸºäºé¢„è®¡ç®—çš„importance
- ä»»åŠ¡æ„ŸçŸ¥ï¼šå¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„importance score

---

## 5. è¾“å…¥ç‰¹å¾è®¾è®¡

### 5.1 Stage 1è¾“å…¥

| ç‰¹å¾ | æå–æ–¹å¼ | ç»´åº¦ | è¯´æ˜ |
|------|---------|------|------|
| Language | Tokenizer + WTE + Mean pooling | (B, d_model) | ä»promptæå– |
| Budget | MLP encoder | (B, hidden_dim) | ä»latency budgetç¼–ç  |

**å…³é”®ç‚¹**ï¼š
- âœ… ä¸éœ€è¦vision featureï¼ˆvisionè¿˜æ²¡å¤„ç†ï¼‰
- âš ï¸ **è®¾è®¡é€‰é¡¹**: å¯ä»¥åªç”¨Budgetï¼ˆé€‰é¡¹Aï¼‰æˆ–Budget+Languageï¼ˆé€‰é¡¹Bï¼‰

### 5.2 Stage 2è¾“å…¥

| ç‰¹å¾ | æå–æ–¹å¼ | ç»´åº¦ | è¯´æ˜ |
|------|---------|------|------|
| Vision | Encoder + Projector + Mean pooling | (B, d_model) | **å¿…é¡»ç»è¿‡projector** |
| Language | Tokenizer + WTE + Mean pooling | (B, d_model) | ä»promptæå– |
| Budget | MLP encoder | (B, hidden_dim) | ä»latency budgetç¼–ç  |

**å…³é”®ç‚¹**ï¼š
- âœ… **Vision featureå¿…é¡»ç»è¿‡projector**ï¼šå› ä¸ºStage 2åœ¨vision encoder+projectorä¹‹å
- âœ… ä½¿ç”¨mean poolingè·å¾—å›ºå®šé•¿åº¦ç‰¹å¾

---

## 6. è®­ç»ƒæ–¹æ³•

### 6.1 æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | Overhead | è®­ç»ƒæ—¶é—´ | æ ·æœ¬æ•ˆç‡ | å‡†ç¡®æ€§ | æ¨èåº¦ |
|------|---------|---------|---------|--------|--------|
| **Lookup Table** | 0 | 0 | N/A | ä¸­ç­‰ | â­â­â­ Baseline |
| **Supervised** | ä½ | å¿« | é«˜ | ä¸­ç­‰ | â­â­â­â­ Baseline |
| **GRPO** | ä½ | ä¸­ç­‰ | **æœ€é«˜** | é«˜ | â­â­â­â­â­ **æ¨è** |

### 6.2 æ¨èæ–¹æ¡ˆï¼šä¸¤é˜¶æ®µè®­ç»ƒ

**Stage 1: Supervised Learning**
- **æ•°æ®æ¥æº**: Core experiment JSONæ–‡ä»¶
- **è®­ç»ƒç›®æ ‡**: å­¦ä¹ latency_budget â†’ tieræ˜ å°„
- **ä¼˜ç‚¹**: ç®€å•ç¨³å®šï¼Œè®­ç»ƒå¿«

**Stage 2: GRPO (Online Training)**
- **æ•°æ®æ¥æº**: Online execution + Latency Estimator
- **è®­ç»ƒç›®æ ‡**: å­¦ä¹ accuracy-latency trade-off
- **ä¼˜ç‚¹**: é«˜æ•ˆæ ·æœ¬åˆ©ç”¨ï¼Œå¯ä»¥å­¦ä¹ å¤æ‚çº¦æŸ
- **å…³é”®ç»„ä»¶**: Latency Estimatorï¼ˆé¿å…batch_size=1é™åˆ¶ï¼‰

### 6.3 Latency Estimator

**è®¾è®¡**: è½»é‡çº§MLPï¼ˆ2-3å±‚ï¼‰

**è¾“å…¥ç‰¹å¾**:
```python
features = [
    vision_tokens,      # Number of vision tokens
    text_tokens,        # Number of text tokens
    output_tokens,      # Expected number of output tokens
    tier_idx,           # Tier index (0=low, 1=medium, 2=high)
    top_k,              # MoE top-K value
    num_active_blocks,  # Number of active transformer blocks
]
```

**è¾“å‡ºé¢„æµ‹**: é˜¶æ®µåˆ†è§£
- T_vision_encoder
- T_projector
- T_LLM_prefill
- T_LLM_decode_per_token

**ç”¨é€”**: åœ¨RLè®­ç»ƒä¸­é¢„ä¼°latencyï¼Œé¿å…batch_size=1é™åˆ¶

---

## 7. Overheadåˆ†æ

### 7.1 Controllerå¼€é”€

**Stage 1 (Knob1)**:
- å‚æ•°é‡: ~10K-50Kï¼ˆå–å†³äºæ˜¯å¦ä½¿ç”¨Language featureï¼‰
- Latency: ~0.01-0.1ms
- å æ¯”: <0.01-0.1%

**Stage 2 (Knob2 & Knob3)**:
- å‚æ•°é‡: ~50K-200K
- Latency: ~0.1ms
- å æ¯”: <0.1%

**Total Controller Overhead**:
- å‚æ•°é‡: ~60K-250K
- Latency: ~0.11-0.2ms
- **å æ¯”: <0.1% of total inference**

### 7.2 èŠ‚çœçš„è®¡ç®—

- é€šè¿‡å‡å°‘top_k: èŠ‚çœ10-30% MoEè®¡ç®—
- é€šè¿‡è·³è¿‡blocks: èŠ‚çœ10-25% Transformerè®¡ç®—
- **Net benefit**: èŠ‚çœçš„è®¡ç®— >> controllerå¼€é”€

### 7.3 é›¶Overheadçš„Knobåº”ç”¨

**Top-Kåº”ç”¨**: ç›´æ¥ä¿®æ”¹å±æ€§ï¼ˆ`block.mlp.top_k = new_value`ï¼‰
- é›¶overhead
- ä¸å½±å“è®¡ç®—å›¾

**Block Maskåº”ç”¨**: ä½¿ç”¨BlockMaskWrapperï¼ˆpass-through for skipped blocksï¼‰
- è·³è¿‡blocksæ—¶åªåšidentity pass-through
- Overheadå¯å¿½ç•¥

---

## 8. å®ç°ç»†èŠ‚

### 8.1 åŠ¨æ€æ”¹å˜top_k

**å¯è¡Œæ€§**: âœ… å®Œå…¨å¯è¡Œï¼Œé›¶overhead

**å®ç°**:
```python
# ç›´æ¥ä¿®æ”¹å±æ€§ï¼ˆæœ€ç®€å•é«˜æ•ˆï¼‰
for i in range(4, 16):
    block = model.transformer.blocks[i]
    if hasattr(block, 'mlp') and hasattr(block.mlp, 'top_k'):
        block.mlp.top_k = new_top_k  # ç›´æ¥ä¿®æ”¹ï¼Œé›¶overhead
```

**å…³é”®ç‚¹**:
- `top_k`æ˜¯æ™®é€šPythonå±æ€§ï¼Œä¸åœ¨è®¡ç®—å›¾ä¸­
- å¯ä»¥ç›´æ¥ä¿®æ”¹ï¼Œä¸å½±å“è®¡ç®—å›¾

### 8.2 Importance-Based Block Selection

**å®ç°**: å‚è§`importance_based_block_selection.py`

**å…³é”®å‡½æ•°**:
```python
def select_blocks_by_importance(
    importance_scores: Dict[int, float],
    num_blocks: int,
) -> List[int]:
    """Select top-N most important blocks."""
    sorted_blocks = sorted(
        importance_scores.items(),
        key=lambda x: x[1],
        reverse=True
    )
    return [block_idx for block_idx, _ in sorted_blocks[:num_blocks]]
```

### 8.3 ä»£ç ç»“æ„

```
experiments/controller/
â”œâ”€â”€ minimal_controller.py              # Minimal-overhead controller
â”œâ”€â”€ two_stage_controller.py            # Two-stage controller (å½“å‰å®ç°)
â”œâ”€â”€ importance_based_block_selection.py  # Block selectionå·¥å…·
â”œâ”€â”€ feature_extractors.py              # ç‰¹å¾æå–
â”œâ”€â”€ latency_estimator.py               # Latency estimator
â””â”€â”€ ...
```

---

## 9. æ€§èƒ½æŒ‡æ ‡

### 9.1 OverheadæŒ‡æ ‡

- **Controller Latency**: <0.2ms
- **Controller Memory**: <1MB (parameters + activations)
- **Controller FLOPs**: <100K operations
- **Relative Overhead**: <0.1% of total inference

### 9.2 EffectivenessæŒ‡æ ‡

- **Accuracy Retention**: >95% (compared to full model)
- **Latency Reduction**: 20-50% (depending on budget)
- **Budget Adherence**: >90% (within budget)
- **Pareto Efficiency**: æ˜¾è‘—æå‡accuracy-latency Pareto frontier

### 9.3 EfficiencyæŒ‡æ ‡

- **Training Time**: <1 day (on 4 GPUs)
- **Sample Efficiency**: <10K samples for convergence
- **Inference Throughput**: >95% of baseline (minimal overhead)

---

## 10. å…³é”®è®¾è®¡å†³ç­–

### 10.1 ä¸¤é˜¶æ®µæ¶æ„

**å†³ç­–**: ä½¿ç”¨ä¸¤é˜¶æ®µé¢„æµ‹æ¶æ„

**ç†ç”±**:
- ç¬¦åˆç³»ç»Ÿæ‰§è¡Œæµç¨‹çº¦æŸ
- Knob1å¿…é¡»åœ¨vision encoderä¹‹å‰å†³å®š
- Knob2 & Knob3å¯ä»¥åœ¨vision encoderä¹‹åå†³å®š

### 10.2 Knob3: Importance-Based Pruning

**å†³ç­–**: ä½¿ç”¨importance-based pruningï¼Œè€Œä¸æ˜¯maské¢„æµ‹

**ç†ç”±**:
- ç®€åŒ–è¾“å‡ºç©ºé—´ï¼ˆ2^16 â†’ 5ï¼‰
- ç¡®å®šæ€§é€‰æ‹©ï¼Œç¨³å®šå¯é 
- æ•°æ®æ— å…³ï¼ˆåŸºäºé¢„è®¡ç®—çš„importanceï¼‰

### 10.3 è®­ç»ƒæ–¹æ³•: GRPO

**å†³ç­–**: ä½¿ç”¨GRPOè¿›è¡ŒStage 2è®­ç»ƒ

**ç†ç”±**:
- Critic-freeï¼Œè®­ç»ƒå¿«
- é«˜æ•ˆæ ·æœ¬åˆ©ç”¨
- å¯ä»¥å­¦ä¹ å¤æ‚çš„accuracy-latency trade-off

### 10.4 AdaLoRA-Inspiredè®¾è®¡ï¼ˆä¸¤ç§æ€è·¯ï¼‰

#### æ€è·¯1: ä¸¤é˜¶æ®µé¢„æµ‹ï¼ˆå½“å‰å®ç°ï¼‰

**æ¶æ„**:
```
Stage 1: Knob1 (Before Vision Encoder)
  - Input: Budget only æˆ– Budget + Language
  - Output: Vision Tokens Tier

â†“ Vision Encoder + Projector

Stage 2: Knob2 & Knob3 (After Projector)
  - Input: Vision + Language + Budget tokens
  - Method: å€Ÿç”¨LLMå‰3å±‚åšattentionèåˆ
  - Output: Top-K + Transformer Blocks (å13å±‚)
```

**ç‰¹ç‚¹**:
- ç¬¦åˆç³»ç»Ÿæ‰§è¡Œæµç¨‹
- åˆ©ç”¨LLMå‰3å±‚çš„è¡¨ç¤ºèƒ½åŠ›
- ä¸¤é˜¶æ®µå†³ç­–ï¼Œæ¸…æ™°æ˜ç¡®

**å®ç°**: å½“å‰`two_stage_controller.py`

#### æ€è·¯2: ä¸€é˜¶æ®µé¢„æµ‹ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

**æ¶æ„**:
```
Single Stage: All Knobs (After Vision Encoder)
  - Input: Budget + Language + Vision (global crop)
  - Method: èåˆæ‰€æœ‰ç‰¹å¾ï¼Œç›´æ¥é¢„æµ‹ä¸‰ä¸ªknob
  - Output: Tier + Top-K + Transformer Blocks
```

**ç‰¹ç‚¹**:
- ä¸€æ¬¡æ€§å†³ç­–ï¼Œæ›´ç®€æ´
- éœ€è¦é¢å¤–çš„vision encoder passï¼ˆglobal cropï¼‰
- å¯ä»¥åˆ©ç”¨å®Œæ•´çš„è§†è§‰ä¿¡æ¯

**å®æ–½è®¡åˆ’**: ä¸¤ç§æ–¹æ¡ˆéƒ½ä¿ç•™ï¼Œåˆ†åˆ«å®ç°å’Œå¯¹æ¯”

### 10.5 å·²ç¡®è®¤å†³ç­–

**Knob1è®¾è®¡é€‰é¡¹**:
- âœ… é€‰é¡¹A: Budget-Onlyï¼ˆæœ€å°overheadï¼‰- ä¼˜å…ˆå®ç°
- âœ… é€‰é¡¹B: Budget + Language - è°ƒç ”Semantic Routeré›†æˆ
- âœ… é€‰é¡¹C: Budget + Language + Vision - éœ€è¦ä¼˜åŒ–å»ºè®®

**Importance Scoreç†è§£**:
- âœ… Data-Agnostic: ä¸æ•°æ®æ¥æºæ— å…³
- âœ… Task-Dependent: ä¸ä»»åŠ¡ç±»å‹ç›¸å…³
- âœ… åº”ç”¨ç­–ç•¥: æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„importance score

---

## é™„å½•

### A. ç›¸å…³æ–‡æ¡£

- `ANALYSIS.md`: æŠ€æœ¯åˆ†ææ–‡æ¡£
- `knob1_predictor_variants.md`: Knob1é¢„æµ‹å™¨çš„ä¸åŒå˜ä½“åˆ†æ
- `controller_implementation_details.md`: å®ç°ç»†èŠ‚å’Œå¯è¡Œæ€§åˆ†æ
- `IMPLEMENTATION_ROADMAP.md`: å®ç°è·¯çº¿å›¾
- `SEMANTIC_ROUTER_INTEGRATION.md`: Semantic Routeré›†æˆè°ƒç ”
- `ADALORA_DESIGNS.md`: AdaLoRA-Inspiredè®¾è®¡çš„ä¸¤ç§æ€è·¯

### B. ä»£ç å®ç°

- `experiments/controller/two_stage_controller.py`: ä¸¤é˜¶æ®µcontrollerå®ç°ï¼ˆæ€è·¯1ï¼‰
- `experiments/controller/minimal_controller.py`: æœ€å°overhead controller
- `experiments/controller/importance_based_block_selection.py`: Block selectionå·¥å…·

### C. å¤–éƒ¨èµ„æº

- **Semantic Router**: [https://github.com/aurelio-labs/semantic-router](https://github.com/aurelio-labs/semantic-router)
  - å¯ç”¨äºKnob1é€‰é¡¹Bçš„å¿«é€Ÿè¯­ä¹‰è·¯ç”±å†³ç­–
  - æ”¯æŒå¤šæ¨¡æ€è·¯ç”±
  - è¶…å¿«é€Ÿå†³ç­–ï¼ˆ<10msï¼‰

### D. è®¾è®¡å†å²

- **v1.0**: åŸå§‹è®¾è®¡ï¼ˆå•é˜¶æ®µï¼‰
- **v2.0**: ä¸¤é˜¶æ®µæ¶æ„ï¼ˆå½“å‰ç‰ˆæœ¬ï¼Œæ€è·¯1ï¼‰
- **v2.1**: SIGMETRICSä¼˜åŒ–ï¼ˆæœ€å°overheadï¼‰
- **v2.2**: æ·»åŠ ä¸€é˜¶æ®µæ–¹æ¡ˆï¼ˆæ€è·¯2ï¼‰å’ŒSemantic Routerè°ƒç ”

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£æ•´åˆäº†æ‰€æœ‰controllerè®¾è®¡æ–‡æ¡£ï¼ŒåŸºäºç°æœ‰ä»£ç å®ç°å’ŒSIGMETRICSæ ‡å‡†ã€‚å¦‚æœ‰ä¸ä¸€è‡´ï¼Œä»¥ä»£ç å®ç°ä¸ºå‡†ã€‚

