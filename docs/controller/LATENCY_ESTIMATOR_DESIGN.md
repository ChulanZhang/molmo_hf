# Latency Estimator è®¾è®¡ä¸ç»“æ„

> **æ–‡æ¡£ç›®çš„**: è¯¦ç»†è¯´æ˜Latency Estimatorçš„æ¶æ„ã€è®¾è®¡åŸç†å’Œä½¿ç”¨æ–¹æ³•  
> **æœ€åæ›´æ–°**: 2026-01-01  
> **ç‰ˆæœ¬**: 2.0 (é‡æ„ç‰ˆ)

## ğŸ“‹ ç›®å½•

1. [è®¾è®¡ç›®æ ‡ä¸ç”¨é€”](#è®¾è®¡ç›®æ ‡ä¸ç”¨é€”)
2. [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)
3. [è¾“å…¥ç‰¹å¾](#è¾“å…¥ç‰¹å¾)
4. [è¾“å‡ºé¢„æµ‹](#è¾“å‡ºé¢„æµ‹)
5. [è®­ç»ƒæ–¹æ³•](#è®­ç»ƒæ–¹æ³•)
6. [ä½¿ç”¨åœºæ™¯](#ä½¿ç”¨åœºæ™¯)
7. [ä»£ç å®ç°](#ä»£ç å®ç°)

---

## ğŸ¯ è®¾è®¡ç›®æ ‡ä¸ç”¨é€”

### ä¸ºä»€ä¹ˆéœ€è¦Latency Estimatorï¼Ÿ

åœ¨Controllerçš„è®­ç»ƒå’Œæ¨ç†ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

1. **é¢„æµ‹æ¯ä¸ªconfigurationçš„latency**: å¯¹äºç»™å®šçš„configurationï¼ˆtier, top_k, num_active_blocksï¼‰ï¼Œé¢„æµ‹å…¶latencyç‰¹å¾
2. **åæ¨æ»¡è¶³budgetçš„configuration set**: ç»™å®šlatency budgetï¼Œæ‰¾å‡ºæ‰€æœ‰æ»¡è¶³budgetçš„configurations
3. **é€‰æ‹©æœ€ä¼˜configuration**: åœ¨æ»¡è¶³budgetçš„configurationsä¸­ï¼Œé€‰æ‹©accuracyæœ€é«˜çš„

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **ä¸ä¾èµ–æœªçŸ¥ä¿¡æ¯**: æ¨ç†æ—¶ä¸çŸ¥é“ä¼šç”Ÿæˆå¤šå°‘ä¸ªtokenï¼Œæ‰€ä»¥**ä¸ä½¿ç”¨output_tokensä½œä¸ºè¾“å…¥**
2. **é¢„æµ‹å¯ç»„åˆçš„latency**: åˆ†åˆ«é¢„æµ‹prefill latencyå’Œdecode per-token latencyï¼Œå¯ä»¥æ ¹æ®å®é™…output_tokensè®¡ç®—æ€»latency
3. **è½»é‡çº§**: å‚æ•°é‡å°ï¼ˆ~100Kï¼‰ï¼Œæ¨ç†å¿«ï¼ˆ<0.1msï¼‰
4. **å‡†ç¡®æ€§**: é¢„æµ‹è¯¯å·® <5msï¼ˆprefillï¼‰ï¼Œ<1msï¼ˆdecode per-tokenï¼‰

### ä½¿ç”¨é€»è¾‘

```
ç»™å®šlatency budget â†’ å¯¹äºæ¯ä¸ªconfiguration:
  1. é¢„æµ‹ T_prefill_total å’Œ T_decode_per_token
  2. è®¡ç®—æ»¡è¶³budgetçš„æœ€å¤§output_tokens:
     max_output_tokens = (budget - T_prefill_total) / T_decode_per_token
  3. å¦‚æœ max_output_tokens > 0ï¼Œè¿™ä¸ªconfigurationæ˜¯å¯è¡Œçš„
  4. åœ¨å¯è¡Œconfigurationsä¸­é€‰æ‹©accuracyæœ€é«˜çš„
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ•´ä½“ç»“æ„

```
Input Features (5ç»´) â†’ Feature Encoder (MLP) â†’ 2ä¸ªé¢„æµ‹å¤´ â†’ è¾“å‡º
```

### è¯¦ç»†æ¶æ„

```python
LatencyEstimator(
    hidden_dim=256,        # éšè—å±‚ç»´åº¦
    num_layers=2,          # MLPå±‚æ•°ï¼ˆä¸åŒ…æ‹¬è¾“å‡ºå±‚ï¼‰
)
```

**å‚æ•°é‡è®¡ç®—**:
- Input â†’ Hidden: `5 Ã— 256 = 1,280`
- Hidden â†’ Hidden: `256 Ã— 256 = 65,536` (æ¯å±‚)
- Hidden â†’ Output: `256 Ã— 1 = 256` (æ¯ä¸ªå¤´)
- LayerNorm: `256 Ã— 2 = 512` (æ¯å±‚)
- **æ€»å‚æ•°é‡**: ~100K-200Kï¼ˆå–å†³äºnum_layersï¼‰

### ç½‘ç»œç»“æ„

```
Input (B, 5)
    â†“
Linear(5 â†’ 256) + LayerNorm + ReLU
    â†“
[å¯é€‰] Linear(256 â†’ 256) + LayerNorm + ReLU  (å¦‚æœnum_layers > 1)
    â†“
Shared Encoder Output (B, 256)
    â†“
    â”œâ”€â†’ Linear(256 â†’ 1) + ReLU â†’ T_prefill_total
    â””â”€â†’ Linear(256 â†’ 1) + ReLU â†’ T_decode_per_token
```

**å…³é”®è®¾è®¡**:
- **å…±äº«ç¼–ç å™¨**: ä¸¤ä¸ªé¢„æµ‹å¤´å…±äº«åŒä¸€ä¸ªç‰¹å¾ç¼–ç å™¨ï¼Œå‡å°‘å‚æ•°é‡
- **ReLUæ¿€æ´»**: ç¡®ä¿è¾“å‡ºéè´Ÿï¼ˆlatencyä¸èƒ½ä¸ºè´Ÿï¼‰
- **LayerNorm**: ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›

---

## ğŸ“¥ è¾“å…¥ç‰¹å¾

### ç‰¹å¾åˆ—è¡¨ï¼ˆ5ç»´ï¼‰

| ç‰¹å¾ | ç±»å‹ | èŒƒå›´/å–å€¼ | è¯´æ˜ |
|------|------|----------|------|
| `vision_tokens` | int | 100-1000 | Vision tokenæ•°é‡ï¼ˆå–å†³äºtierå’Œå›¾åƒå¤§å°ï¼‰ |
| `text_tokens` | int | 20-200 | æ–‡æœ¬tokenæ•°é‡ï¼ˆprompté•¿åº¦ï¼‰ |
| `tier_idx` | int | 0, 1, 2 | Tierç´¢å¼•ï¼ˆ0=low, 1=medium, 2=highï¼‰ |
| `top_k` | int | 4, 6, 8, 10, 12 | MoE top-Kå€¼ |
| `num_active_blocks` | int | 8, 10, 12, 14, 16 | æ¿€æ´»çš„transformer blockæ•°é‡ |

**é‡è¦**: **ä¸ä½¿ç”¨output_tokensä½œä¸ºè¾“å…¥**ï¼Œå› ä¸ºæ¨ç†æ—¶ä¸çŸ¥é“ä¼šç”Ÿæˆå¤šå°‘ä¸ªtokenã€‚

### ç‰¹å¾ç¼–ç 

```python
# æ„å»ºç‰¹å¾å‘é‡
features = torch.stack([
    vision_tokens.float(),      # (B,)
    text_tokens.float(),        # (B,)
    tier_idx.float(),           # (B,)
    top_k.float(),              # (B,)
    num_active_blocks.float(),  # (B,)
], dim=-1)  # (B, 5)
```

---

## ğŸ“¤ è¾“å‡ºé¢„æµ‹

### é¢„æµ‹ç›®æ ‡ï¼ˆ2ä¸ªï¼‰

| è¾“å‡º | ç±»å‹ | å•ä½ | è¯´æ˜ |
|------|------|------|------|
| `T_prefill_total` | float | ms | æ€»prefill latency = Vision encoder + Projector + LLM prefill |
| `T_decode_per_token` | float | ms/token | æ¯ä¸ªè¾“å‡ºtokençš„decode latency |

### è¾“å‡ºè®¡ç®—æµç¨‹

```python
# 1. é¢„æµ‹latency
T_prefill_total = prefill_head(encoded)      # (B,)
T_decode_per_token = decode_head(encoded)    # (B,)

# 2. ä½¿ç”¨æ—¶çš„æ€»latencyè®¡ç®—ï¼ˆåœ¨å¤–éƒ¨ï¼‰
# T_total = T_prefill_total + T_decode_per_token * output_tokens
```

**è®¾è®¡è€ƒè™‘**:
- **é˜¶æ®µåˆ†è§£**: åˆ†åˆ«é¢„æµ‹prefillå’Œdecode per-tokenï¼Œå¯ä»¥æ ¹æ®å®é™…output_tokensè®¡ç®—æ€»latency
- **ä¸é¢„æµ‹T_total**: å› ä¸ºoutput_tokensæœªçŸ¥ï¼Œæ— æ³•åœ¨estimatorå†…éƒ¨è®¡ç®—T_total
- **å¯ç»„åˆæ€§**: é¢„æµ‹çš„latencyå¯ä»¥çµæ´»ç»„åˆï¼Œé€‚åº”ä¸åŒçš„output_tokens

### ä½¿ç”¨ç¤ºä¾‹

```python
# é¢„æµ‹latency
latencies = estimator(
    vision_tokens=vision_tokens,
    text_tokens=text_tokens,
    tier_idx=tier_idx,
    top_k=top_k,
    num_active_blocks=num_active_blocks,
)

# æ£€æŸ¥æ˜¯å¦æ»¡è¶³budgetï¼ˆå‡è®¾expected_output_tokensï¼‰
T_prefill = latencies['T_prefill_total']
T_decode_per_token = latencies['T_decode_per_token']
T_total = T_prefill + T_decode_per_token * expected_output_tokens

if T_total <= latency_budget:
    # Configurationå¯è¡Œ
    pass
```

---

## ğŸ“ è®­ç»ƒæ–¹æ³•

### è®­ç»ƒæ•°æ®

**æ•°æ®æ¥æº**: Core experimentç»“æœï¼ˆJSONæ–‡ä»¶ï¼‰

**æ•°æ®æ ¼å¼**:
```json
{
  "per_sample_results": [
    {
      "actual_vision_tokens": 384,
      "actual_text_tokens": 53,
      "output_tokens": 11,
      "tier": "low",
      "top_k": 4,
      "num_active_blocks": 12,
      "T_vision_total": 11.93,
      "T_LLM_prefill": 99.25,
      "T_LLM_decode": 38.85,
      "T_decode_per_token": 19.43
    }
  ]
}
```

**æ•°æ®é¢„å¤„ç†**:
- `T_prefill_total = T_vision_total + T_LLM_prefill`
- `T_decode_per_token = T_LLM_decode / output_tokens`ï¼ˆå¦‚æœJSONä¸­æ²¡æœ‰ï¼Œåˆ™è®¡ç®—ï¼‰

### æŸå¤±å‡½æ•°

**å¤šä»»åŠ¡æŸå¤±**:
```python
loss = loss_prefill + loss_decode
```

å…¶ä¸­æ¯ä¸ªlosséƒ½æ˜¯MSE loss:
- `loss_prefill = MSE(pred_T_prefill_total, target_T_prefill_total)`
- `loss_decode = MSE(pred_T_decode_per_token, target_T_decode_per_token)`

**è®¾è®¡è€ƒè™‘**:
- **åªè®­ç»ƒä¸¤ä¸ªç›®æ ‡**: ä¸è®­ç»ƒT_totalï¼Œå› ä¸ºoutput_tokensåœ¨è®­ç»ƒæ—¶å·²çŸ¥ï¼Œä½†åœ¨æ¨ç†æ—¶æœªçŸ¥
- **ç­‰æƒé‡**: ä¸¤ä¸ªlossç­‰æƒé‡ï¼Œå› ä¸ºéƒ½æ˜¯é‡è¦çš„latencyç»„ä»¶

### è®­ç»ƒæŒ‡æ ‡

**ä¸»è¦æŒ‡æ ‡**:
- **MAE (Mean Absolute Error)**: å¹³å‡ç»å¯¹è¯¯å·®
  - `MAE_prefill < 5ms`
  - `MAE_decode_per_token < 1ms`
- **Relative Error**: ç›¸å¯¹è¯¯å·®
  - `rel_error_prefill < 5%`
  - `rel_error_decode < 10%`

### è®­ç»ƒé…ç½®

```python
# é»˜è®¤é…ç½®
batch_size = 64
num_epochs = 50
lr = 1e-3
weight_decay = 1e-5
optimizer = Adam
train_split = 0.8  # 80%è®­ç»ƒï¼Œ20%éªŒè¯
```

---

## ğŸš€ ä½¿ç”¨åœºæ™¯

### 1. åæ¨æ»¡è¶³budgetçš„configuration set

```python
# ç»™å®šlatency budgetå’Œexpected_output_tokens
latency_budget = 200.0  # ms
expected_output_tokens = 10  # å‡è®¾å€¼

# æšä¸¾æ‰€æœ‰å¯èƒ½çš„configurations
configs = [
    {'tier': 'low', 'top_k': 4, 'num_active_blocks': 8},
    {'tier': 'low', 'top_k': 6, 'num_active_blocks': 10},
    # ...
]

feasible_configs = []
for config in configs:
    # é¢„æµ‹latency
    latencies = estimator.predict_from_config({
        'vision_tokens': vision_tokens,
        'text_tokens': text_tokens,
        'tier': config['tier'],
        'top_k': config['top_k'],
        'num_active_blocks': config['num_active_blocks'],
    })
    
    # è®¡ç®—æ€»latency
    T_total = latencies['T_prefill_total'] + latencies['T_decode_per_token'] * expected_output_tokens
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³budget
    if T_total <= latency_budget:
        feasible_configs.append(config)
```

### 2. Controllerè®­ç»ƒåŠ é€Ÿï¼ˆGRPOï¼‰

åœ¨GRPOè®­ç»ƒä¸­ï¼Œä½¿ç”¨estimatoré¢„æµ‹latencyï¼Œé¿å…å®é™…è¿è¡Œæ¨¡å‹ï¼š

```python
# ä¼ ç»Ÿæ–¹æ³•ï¼ˆæ…¢ï¼‰
latency = run_model(config)  # batch_size=1, å¾ˆæ…¢

# ä½¿ç”¨estimatorï¼ˆå¿«ï¼‰
latencies = estimator.predict(config)
T_total = latencies['T_prefill_total'] + latencies['T_decode_per_token'] * expected_output_tokens
```

**ä¼˜åŠ¿**:
- æ”¯æŒbatché¢„æµ‹ï¼ˆbatch_size > 1ï¼‰
- æ¨ç†é€Ÿåº¦å¿«ï¼ˆ<0.1ms vs 100ms+ï¼‰
- è®­ç»ƒé€Ÿåº¦æå‡10-100å€

### 3. é…ç½®æœç´¢

åœ¨controllerè®­ç»ƒå‰ï¼Œå¯ä»¥ä½¿ç”¨estimatorå¿«é€Ÿè¯„ä¼°ä¸åŒé…ç½®ï¼š

```python
configs = [
    {'tier': 'low', 'top_k': 4, 'num_active_blocks': 8},
    {'tier': 'medium', 'top_k': 6, 'num_active_blocks': 10},
    # ...
]

for config in configs:
    latencies = estimator.predict_from_config(config)
    print(f"Config {config}: prefill={latencies['T_prefill_total']:.2f}ms, decode_per_token={latencies['T_decode_per_token']:.3f}ms/token")
```

---

## ğŸ’» ä»£ç å®ç°

### æ¨¡å‹å®šä¹‰

**æ–‡ä»¶**: `experiments/controller/latency_estimator.py`

**æ ¸å¿ƒç±»**:
- `LatencyEstimator`: æ¨¡å‹å®šä¹‰
- `LatencyEstimatorTrainer`: è®­ç»ƒå™¨

### ä½¿ç”¨ç¤ºä¾‹

**è®­ç»ƒ**:
```python
from experiments.controller.latency_estimator import LatencyEstimator, LatencyEstimatorTrainer

# åˆ›å»ºæ¨¡å‹
model = LatencyEstimator(hidden_dim=256, num_layers=2)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = LatencyEstimatorTrainer(model, device='cuda', lr=1e-3)

# è®­ç»ƒ
for epoch in range(num_epochs):
    for batch in train_loader:
        metrics = trainer.train_step(batch)
```

**æ¨ç†**:
```python
# æ–¹æ³•1: ç›´æ¥forward
latencies = model(
    vision_tokens=torch.tensor([384]),
    text_tokens=torch.tensor([53]),
    tier_idx=torch.tensor([0]),  # low
    top_k=torch.tensor([4]),
    num_active_blocks=torch.tensor([12]),
)

# æ–¹æ³•2: ä½¿ç”¨predict_from_config
config = {
    'vision_tokens': torch.tensor([384]),
    'text_tokens': torch.tensor([53]),
    'tier': ['low'],  # æˆ– torch.tensor([0])
    'top_k': torch.tensor([4]),
    'num_active_blocks': torch.tensor([12]),
}
latencies = model.predict_from_config(config)

# è®¡ç®—æ€»latencyï¼ˆéœ€è¦output_tokensï¼‰
output_tokens = 10
T_total = latencies['T_prefill_total'] + latencies['T_decode_per_token'] * output_tokens
```

**æ£€æŸ¥budgetå¯è¡Œæ€§**:
```python
# ä½¿ç”¨check_budget_feasibilityæ–¹æ³•
feasible = model.check_budget_feasibility(
    vision_tokens=vision_tokens,
    text_tokens=text_tokens,
    tier_idx=tier_idx,
    top_k=top_k,
    num_active_blocks=num_active_blocks,
    latency_budget=latency_budget,
    expected_output_tokens=expected_output_tokens,
)
```

### è®­ç»ƒè„šæœ¬

**æ–‡ä»¶**: `experiments/controller/train_latency_estimator.py`

**ä½¿ç”¨**:
```bash
# ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®é›†ï¼ˆæ¨èï¼‰
python experiments/controller/train_latency_estimator.py \
    --results_dir results/core_exp_h100/4run_2000samples \
    --use_all_datasets \
    --output_dir checkpoints/latency_estimator \
    --batch_size 64 \
    --num_epochs 50 \
    --lr 1e-3 \
    --device cuda \
    --seed 3407

# æˆ–è€…ä¸æŒ‡å®šdataset_namesï¼Œä¼šè‡ªåŠ¨æ£€æµ‹æ‰€æœ‰æ•°æ®é›†
python experiments/controller/train_latency_estimator.py \
    --results_dir results/core_exp_h100/4run_2000samples \
    --output_dir checkpoints/latency_estimator \
    --batch_size 64 \
    --num_epochs 50 \
    --lr 1e-3 \
    --device cuda \
    --seed 3407
```

**å¯ç”¨æ•°æ®é›†**ï¼ˆåœ¨`4run_2000samples`ç›®å½•ä¸‹ï¼‰:
- `coco_2014_vqa`
- `coco_caption`
- `doc_qa`
- `mmmu`
- `okvqa`
- `science_qa_img`
- `st_qa`
- `tally_qa`
- `text_vqa`

æ€»å…±9ä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†æœ‰27ä¸ªJSONæ–‡ä»¶ï¼ˆä¸åŒé…ç½®ç»„åˆï¼‰ã€‚

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### é¢„æœŸæ€§èƒ½

**å‚æ•°é‡**: ~100K-200K parameters

**æ¨ç†é€Ÿåº¦**: <0.1ms per sample (GPU)

**é¢„æµ‹å‡†ç¡®åº¦**:
- **MAE_prefill**: <5ms
- **MAE_decode_per_token**: <1ms
- **Relative Error**: <5% (prefill), <10% (decode)

### å®é™…æ€§èƒ½ï¼ˆå¾…è®­ç»ƒåéªŒè¯ï¼‰

è®­ç»ƒå®Œæˆåï¼Œä¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼š
- å„é˜¶æ®µçš„MAEå’ŒRMSE
- ä¸åŒé…ç½®ä¸‹çš„è¯¯å·®åˆ†å¸ƒ
- ç›¸å¯¹è¯¯å·®åˆ†æ

---

## ğŸ”§ è®¾è®¡ç»†èŠ‚

### 1. ä¸ºä»€ä¹ˆåªé¢„æµ‹prefillå’Œdecode per-tokenï¼Ÿ

**åŸå› **:
- æ¨ç†æ—¶ä¸çŸ¥é“output_tokensï¼Œæ— æ³•é¢„æµ‹T_total
- åˆ†åˆ«é¢„æµ‹prefillå’Œdecode per-tokenï¼Œå¯ä»¥æ ¹æ®å®é™…output_tokensçµæ´»è®¡ç®—T_total
- æ›´ç¬¦åˆå®é™…ä½¿ç”¨åœºæ™¯

### 2. ä¸ºä»€ä¹ˆä¸ç”¨output_tokensä½œä¸ºè¾“å…¥ï¼Ÿ

**åŸå› **:
- æ¨ç†æ—¶ä¸çŸ¥é“ä¼šç”Ÿæˆå¤šå°‘ä¸ªtoken
- å¦‚æœä½¿ç”¨output_tokensä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒå’Œæ¨ç†çš„æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´
- ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ¨ç†æ—¶æ— æ³•ä½¿ç”¨

### 3. ä¸ºä»€ä¹ˆä½¿ç”¨å…±äº«ç¼–ç å™¨ï¼Ÿ

**åŸå› **:
- å‡å°‘å‚æ•°é‡ï¼ˆ2ä¸ªç‹¬ç«‹ç¼–ç å™¨ vs 1ä¸ªå…±äº«ç¼–ç å™¨ï¼‰
- å¤šä»»åŠ¡å­¦ä¹ ï¼Œå…±äº«ç‰¹å¾è¡¨ç¤º
- è®­ç»ƒæ›´ç¨³å®š

### 4. å¦‚ä½•è®¡ç®—T_prefill_totalï¼Ÿ

**è®¡ç®—æ–¹å¼**:
- `T_prefill_total = T_vision_total + T_LLM_prefill`
- åœ¨è®­ç»ƒæ•°æ®é¢„å¤„ç†æ—¶è®¡ç®—
- è¿™æ ·é¢„æµ‹ä¸€ä¸ªå€¼å³å¯ï¼Œä¸éœ€è¦åˆ†åˆ«é¢„æµ‹visionå’Œprefill

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **[DESIGN.md](DESIGN.md)**: Controlleræ•´ä½“è®¾è®¡
- **[EXPERIMENTS.md](EXPERIMENTS.md)**: å®éªŒè¯´æ˜ï¼ˆExp 1ï¼‰
- **[ANALYSIS.md](ANALYSIS.md)**: æŠ€æœ¯åˆ†æ

---

**æœ€åæ›´æ–°**: 2026-01-01  
**ç»´æŠ¤è€…**: Controller Team
