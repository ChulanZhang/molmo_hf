# GRPO Controller 设计总结

## 核心问题

训练一个智能控制器，根据输入图像和语言特征以及延迟预算，动态选择最优的模型配置（max_crops, top_k, num_active_blocks），在满足延迟约束的前提下最大化准确率。

## GRPO方法选择理由

### GRPO原理

GRPO（Group Relative Policy Optimization）通过组内轨迹的相对比较来估计优势函数，无需学习独立的价值函数：

```
A_relative(s_i, a_i) = R_i - R_group_mean
```

**核心优势**：
1. **简化训练**：不需要价值网络，减少一半参数
2. **提高稳定性**：相对比较减少方差
3. **适合离线学习**：可以直接利用exp5/exp6的离线数据
4. **自动归一化**：组内比较自动处理不同延迟预算下的奖励尺度问题

### 与其他方法比较

| 方法 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **监督学习** | 简单直接 | 需要大量标注，难以适应动态预算 | 不适合 |
| **PPO** | 成熟稳定 | 需要价值网络，训练复杂 | 在线学习场景 |
| **DPO** | 直接优化 | 可能不稳定，需要成对数据 | 偏好学习 |
| **GRPO** | 稳定、简单、适合离线 | 相对较新 | **✓ 最适合** |

## 系统架构

### Controller位置

```
Input → Controller → Config Selection → Model Inference → Output
```

**位置**：Pre-inference，作为独立决策模块

**优点**：
- 低耦合，易训练和部署
- 可以灵活替换策略
- 不影响主模型结构

### 特征提取

1. **图像特征**：使用vision backbone的CLS token或mean pooling
2. **语言特征**：使用language encoder的mean pooling
3. **延迟预算**：标量值，使用线性编码或sinusoidal encoding

### Controller网络

- **输入**：图像特征(768D) + 语言特征(2048D) + 延迟预算(32D)
- **架构**：轻量级MLP（hidden_dim=256）
- **输出**：三个独立的动作分布（max_crops, top_k, num_active_blocks）
- **参数量**：约500K（远小于主模型）

### Overhead控制

**目标**：< 5% 总推理时间

**策略**：
1. 轻量级网络（256 hidden dim）
2. 特征复用（从主模型forward中提取）
3. 量化优化（INT8）
4. 批处理支持

## Reward函数设计

### 核心公式

```python
reward = α·accuracy 
       - β·latency_penalty 
       - γ·budget_violation_penalty 
       - ε·complexity_penalty 
       + δ·efficiency_bonus
```

### 关键特性

1. **硬约束**：预算违反惩罚（γ=10.0）确保满足延迟要求
2. **平滑惩罚**：使用sigmoid函数平滑延迟惩罚
3. **效率奖励**：在预算内时，延迟越小越好
4. **复杂度惩罚**：鼓励选择简单配置（可选）

### 默认参数

- α = 1.0（准确率权重）
- β = 0.5（延迟惩罚）
- γ = 10.0（预算违反惩罚，硬约束）
- δ = 0.1（效率奖励）
- ε = 0.05（复杂度惩罚）

## 训练流程

### 1. 数据准备

从exp5和exp6结果中提取：
- 配置信息（max_crops, top_k, num_active_blocks）
- 准确率
- 延迟（exp6）
- 图像和语言特征（需要重新提取）

### 2. GRPO训练

1. **分组**：按延迟预算分组，每组8个样本
2. **计算优势**：组内相对优势 = R_i - R_group_mean
3. **策略更新**：∇θ J = E[∇θ log π_θ(a|s) · A_relative]

### 3. 评估

- 准确率提升
- 延迟满足率
- 平均延迟
- Controller开销

## 实施计划

### 阶段1：数据准备（1-2周）
- [x] 解析exp5/exp6结果
- [x] 特征提取模块
- [x] 数据组织

### 阶段2：模型实现（1周）
- [x] Controller网络
- [x] Reward函数
- [x] GRPO训练器

### 阶段3：训练（2-3周）
- [ ] 超参数调优
- [ ] 训练和验证
- [ ] 模型评估

### 阶段4：集成（1-2周）
- [ ] 集成到主模型
- [ ] Overhead优化
- [ ] 端到端测试

### 阶段5：部署（1周）
- [ ] 多数据集评估
- [ ] 性能分析
- [ ] 部署准备

## 关键技术点

### 1. 特征提取效率

**问题**：特征提取可能成为瓶颈

**解决**：
- 在主模型forward时同时提取
- 缓存特征避免重复计算
- 使用轻量级投影层

### 2. 数据不平衡

**问题**：不同配置的数据量可能不平衡

**解决**：
- 使用加权采样
- 数据增强
- 平衡采样策略

### 3. 泛化能力

**问题**：可能过拟合到训练数据

**解决**：
- 正则化（dropout, weight decay）
- 交叉验证
- 多数据集训练

### 4. 延迟预算适应性

**问题**：需要适应不同的延迟预算

**解决**：
- 在训练数据中为每个样本生成多个预算变体
- 使用预算编码让模型学习预算-配置映射

## 预期效果

### 性能指标

- **准确率提升**：相比固定配置提升2-5%
- **延迟满足率**：> 95%
- **平均延迟**：接近但不超过预算
- **Controller开销**：< 5% 总推理时间

### 配置选择

Controller应该能够：
- 简单样本选择简单配置（低延迟）
- 复杂样本选择复杂配置（高准确率）
- 根据预算动态调整

## 风险和缓解

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| 数据不足 | 高 | 数据增强，多数据集 |
| 特征提取开销 | 中 | 特征缓存，异步提取 |
| 训练不稳定 | 中 | 正则化，梯度裁剪 |
| 过拟合 | 中 | 交叉验证，早停 |

## 未来改进

1. **在线学习**：支持实时更新
2. **多任务学习**：同时优化多个数据集
3. **元学习**：快速适应新预算
4. **可解释性**：分析决策过程
5. **Reward Model**：学习更复杂的reward函数

## 参考资料

- GRPO论文（如适用）
- PPO: Proximal Policy Optimization Algorithms
- DPO: Direct Preference Optimization
- 强化学习基础教材

## 联系方式

如有问题或建议，请参考主设计文档 `GRPO_CONTROLLER_DESIGN.md`。

